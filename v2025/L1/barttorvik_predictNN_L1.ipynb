{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9523fa-1bda-4ae8-9aa7-7010e9d1a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict NN file\n",
    "#grabbed torvik data from 2016-24 (no 20 / covid)\n",
    "#looking for correlations in column fields that would predict the elite 8 flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d448845-d905-48d3-b435-4fa14a7132fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Team</th>\n",
       "      <th>Seed</th>\n",
       "      <th>Win</th>\n",
       "      <th>AdjOE</th>\n",
       "      <th>AdjDE</th>\n",
       "      <th>Barthag</th>\n",
       "      <th>AdjOD</th>\n",
       "      <th>EFG%</th>\n",
       "      <th>EFGD%</th>\n",
       "      <th>...</th>\n",
       "      <th>Conf_SC</th>\n",
       "      <th>Conf_SEC</th>\n",
       "      <th>Conf_SWAC</th>\n",
       "      <th>Conf_Slnd</th>\n",
       "      <th>Conf_Sum</th>\n",
       "      <th>Conf_WAC</th>\n",
       "      <th>Conf_WCC</th>\n",
       "      <th>Conf_P12</th>\n",
       "      <th>bluePower_Barthag</th>\n",
       "      <th>bluePower_Seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Houston</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>4.834693</td>\n",
       "      <td>4.488636</td>\n",
       "      <td>0.684258</td>\n",
       "      <td>1</td>\n",
       "      <td>3.983413</td>\n",
       "      <td>3.826465</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.503468</td>\n",
       "      <td>6.225206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Duke</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>3.465736</td>\n",
       "      <td>4.863681</td>\n",
       "      <td>4.525044</td>\n",
       "      <td>0.683450</td>\n",
       "      <td>1</td>\n",
       "      <td>4.067316</td>\n",
       "      <td>3.817712</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.501694</td>\n",
       "      <td>6.225206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>3.367296</td>\n",
       "      <td>4.867534</td>\n",
       "      <td>4.550714</td>\n",
       "      <td>0.680872</td>\n",
       "      <td>1</td>\n",
       "      <td>4.037774</td>\n",
       "      <td>3.850148</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.496029</td>\n",
       "      <td>6.225206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Florida</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>4.857484</td>\n",
       "      <td>4.553877</td>\n",
       "      <td>0.678693</td>\n",
       "      <td>1</td>\n",
       "      <td>4.025352</td>\n",
       "      <td>3.835142</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.491241</td>\n",
       "      <td>6.225206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>4.856707</td>\n",
       "      <td>4.578826</td>\n",
       "      <td>0.674015</td>\n",
       "      <td>0</td>\n",
       "      <td>4.048301</td>\n",
       "      <td>3.889777</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.480963</td>\n",
       "      <td>6.092000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     Team      Seed       Win     AdjOE     AdjDE   Barthag  \\\n",
       "0           0  Houston  2.833213  3.433987  4.834693  4.488636  0.684258   \n",
       "1           2     Duke  2.833213  3.465736  4.863681  4.525044  0.683450   \n",
       "2           4   Auburn  2.833213  3.367296  4.867534  4.550714  0.680872   \n",
       "3           6  Florida  2.833213  3.433987  4.857484  4.553877  0.678693   \n",
       "4           8  Alabama  2.772589  3.258097  4.856707  4.578826  0.674015   \n",
       "\n",
       "   AdjOD      EFG%     EFGD%  ...  Conf_SC  Conf_SEC  Conf_SWAC  Conf_Slnd  \\\n",
       "0      1  3.983413  3.826465  ...        0         0          0          0   \n",
       "1      1  4.067316  3.817712  ...        0         0          0          0   \n",
       "2      1  4.037774  3.850148  ...        0         1          0          0   \n",
       "3      1  4.025352  3.835142  ...        0         1          0          0   \n",
       "4      0  4.048301  3.889777  ...        0         1          0          0   \n",
       "\n",
       "   Conf_Sum  Conf_WAC  Conf_WCC  Conf_P12  bluePower_Barthag  bluePower_Seed  \n",
       "0         0         0         0         0           1.503468        6.225206  \n",
       "1         0         0         0         0           1.501694        6.225206  \n",
       "2         0         0         0         0           1.496029        6.225206  \n",
       "3         0         0         0         0           1.491241        6.225206  \n",
       "4         0         0         0         0           1.480963        6.092000  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68 entries, 0 to 67\n",
      "Data columns (total 58 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         68 non-null     int64  \n",
      " 1   Team               68 non-null     object \n",
      " 2   Seed               68 non-null     float64\n",
      " 3   Win                68 non-null     float64\n",
      " 4   AdjOE              68 non-null     float64\n",
      " 5   AdjDE              68 non-null     float64\n",
      " 6   Barthag            68 non-null     float64\n",
      " 7   AdjOD              68 non-null     int64  \n",
      " 8   EFG%               68 non-null     float64\n",
      " 9   EFGD%              68 non-null     float64\n",
      " 10  TOR                68 non-null     float64\n",
      " 11  TORD               68 non-null     float64\n",
      " 12  ORB                68 non-null     float64\n",
      " 13  DRB                68 non-null     float64\n",
      " 14  FTR                68 non-null     float64\n",
      " 15  FTRD               68 non-null     float64\n",
      " 16  2P%                68 non-null     float64\n",
      " 17  2P%D               68 non-null     float64\n",
      " 18  3P%                68 non-null     float64\n",
      " 19  3P%D               68 non-null     float64\n",
      " 20  3PR                68 non-null     float64\n",
      " 21  3PRD               68 non-null     float64\n",
      " 22  Adj T.             68 non-null     float64\n",
      " 23  bluePower          68 non-null     float64\n",
      " 24  Conf_A10           68 non-null     int64  \n",
      " 25  Conf_ACC           68 non-null     int64  \n",
      " 26  Conf_AE            68 non-null     int64  \n",
      " 27  Conf_ASun          68 non-null     int64  \n",
      " 28  Conf_Amer          68 non-null     int64  \n",
      " 29  Conf_B10           68 non-null     int64  \n",
      " 30  Conf_B12           68 non-null     int64  \n",
      " 31  Conf_BE            68 non-null     int64  \n",
      " 32  Conf_BSky          68 non-null     int64  \n",
      " 33  Conf_BSth          68 non-null     int64  \n",
      " 34  Conf_BW            68 non-null     int64  \n",
      " 35  Conf_CAA           68 non-null     int64  \n",
      " 36  Conf_CUSA          68 non-null     int64  \n",
      " 37  Conf_Horz          68 non-null     int64  \n",
      " 38  Conf_Ivy           68 non-null     int64  \n",
      " 39  Conf_MAAC          68 non-null     int64  \n",
      " 40  Conf_MAC           68 non-null     int64  \n",
      " 41  Conf_MEAC          68 non-null     int64  \n",
      " 42  Conf_MVC           68 non-null     int64  \n",
      " 43  Conf_MWC           68 non-null     int64  \n",
      " 44  Conf_NEC           68 non-null     int64  \n",
      " 45  Conf_OVC           68 non-null     int64  \n",
      " 46  Conf_Pat           68 non-null     int64  \n",
      " 47  Conf_SB            68 non-null     int64  \n",
      " 48  Conf_SC            68 non-null     int64  \n",
      " 49  Conf_SEC           68 non-null     int64  \n",
      " 50  Conf_SWAC          68 non-null     int64  \n",
      " 51  Conf_Slnd          68 non-null     int64  \n",
      " 52  Conf_Sum           68 non-null     int64  \n",
      " 53  Conf_WAC           68 non-null     int64  \n",
      " 54  Conf_WCC           68 non-null     int64  \n",
      " 55  Conf_P12           68 non-null     int64  \n",
      " 56  bluePower_Barthag  68 non-null     float64\n",
      " 57  bluePower_Seed     68 non-null     float64\n",
      "dtypes: float64(23), int64(34), object(1)\n",
      "memory usage: 30.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from IPython.display import display  # Import display for Jupyter Notebooks\n",
    "\n",
    "# ‚úÖ Load datasets\n",
    "df_new = pd.read_csv(\"data/barttorvik_predict_L1.csv\")\n",
    "\n",
    "display(df_new.head(5))\n",
    "display(df_new.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f27b41-15db-482a-aa44-7082c2238554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final df_new shape: (68, 42)\n",
      "‚úÖ Expected number of features: 42 (Matches training data?) True\n",
      "\n",
      "‚úÖ Features in df_new BEFORE SCALING:\n",
      "['Seed', 'Win', 'AdjOE', 'AdjDE', 'Barthag', 'AdjOD', 'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P%', '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.', 'bluePower', 'Conf_A10', 'Conf_ACC', 'Conf_ASun', 'Conf_Amer', 'Conf_B10', 'Conf_B12', 'Conf_BE', 'Conf_CAA', 'Conf_CUSA', 'Conf_MAAC', 'Conf_MVC', 'Conf_MWC', 'Conf_OVC', 'Conf_SB', 'Conf_SC', 'Conf_SEC', 'Conf_WCC', 'Conf_P12', 'bluePower_Barthag', 'bluePower_Seed']\n",
      "\n",
      "‚úÖ Features EXPECTED by Scaler:\n",
      "['Final 4', 'Seed', 'Win', 'AdjOE', 'AdjOD', 'AdjDE', 'Barthag', 'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P%', '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.', 'bluePower', 'Conf_A10', 'Conf_ACC', 'Conf_ASun', 'Conf_Amer', 'Conf_B10', 'Conf_B12', 'Conf_BE', 'Conf_BSky', 'Conf_CUSA', 'Conf_MVC', 'Conf_MWC', 'Conf_NEC', 'Conf_P12', 'Conf_SC', 'Conf_SEC', 'Conf_SWAC', 'Conf_WAC', 'Conf_WCC', 'bluePower_Barthag', 'bluePower_Seed']\n"
     ]
    }
   ],
   "source": [
    "# Keep \"Team\" for final output\n",
    "df_new_team = df_new[[\"Team\"]].copy()\n",
    "\n",
    "# ‚úÖ Drop non-feature columns like \"Team\", \"Unnamed: 0\"\n",
    "df_new = df_new.drop(columns=[\"Team\", \"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# ‚úÖ NEW: Drop the same 14 lowest-importance features that were removed during training\n",
    "features_to_drop = [\n",
    "    \"Conf_Slnd\", \"Conf_AE\", \"Conf_WAC\", \"Conf_SWAC\", \"Conf_Sum\",\n",
    "    \"Conf_NEC\", \"Conf_Pat\", \"Conf_BSky\", \"Conf_MEAC\", \"Conf_MAC\",\n",
    "    \"Conf_Ivy\", \"Conf_Horz\", \"Conf_BW\", \"Conf_BSth\"\n",
    "]\n",
    "df_new = df_new.drop(columns=features_to_drop, errors=\"ignore\")  # Drop extra features\n",
    "\n",
    "# ‚úÖ Load trained scaler FIRST\n",
    "scaler = joblib.load(\"../L2/data/elite8_scaler.pkl\")\n",
    "\n",
    "# ‚úÖ Print shape to confirm it still matches training data\n",
    "print(f\"‚úÖ Final df_new shape: {df_new.shape}\")\n",
    "print(f\"‚úÖ Expected number of features: 42 (Matches training data?) {df_new.shape[1] == 42}\")\n",
    "\n",
    "# ‚úÖ Debugging Step: Print feature names before scaling\n",
    "print(\"\\n‚úÖ Features in df_new BEFORE SCALING:\")\n",
    "print(df_new.columns.tolist())\n",
    "\n",
    "print(\"\\n‚úÖ Features EXPECTED by Scaler:\")\n",
    "print(scaler.feature_names_in_.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0093c0b-ba3e-498f-95b8-9be2ec797d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Features in df_new BEFORE selection:\n",
      "['Seed', 'Win', 'AdjOE', 'AdjDE', 'Barthag', 'AdjOD', 'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P%', '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.', 'bluePower', 'Conf_A10', 'Conf_ACC', 'Conf_ASun', 'Conf_Amer', 'Conf_B10', 'Conf_B12', 'Conf_BE', 'Conf_CAA', 'Conf_CUSA', 'Conf_MAAC', 'Conf_MVC', 'Conf_MWC', 'Conf_OVC', 'Conf_SB', 'Conf_SC', 'Conf_SEC', 'Conf_WCC', 'Conf_P12', 'bluePower_Barthag', 'bluePower_Seed']\n",
      "\n",
      "üö® Dropping extra features: ['Conf_CAA', 'Conf_MAAC', 'Conf_OVC', 'Conf_SB']\n",
      "\n",
      "‚úÖ Features in df_new AFTER dropping extras:\n",
      "['Seed', 'Win', 'AdjOE', 'AdjDE', 'Barthag', 'AdjOD', 'EFG%', 'EFGD%', 'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P%', '2P%D', '3P%', '3P%D', '3PR', '3PRD', 'Adj T.', 'bluePower', 'Conf_A10', 'Conf_ACC', 'Conf_ASun', 'Conf_Amer', 'Conf_B10', 'Conf_B12', 'Conf_BE', 'Conf_CUSA', 'Conf_MVC', 'Conf_MWC', 'Conf_SC', 'Conf_SEC', 'Conf_WCC', 'Conf_P12', 'bluePower_Barthag', 'bluePower_Seed']\n",
      "‚úÖ Final df_new shape after feature enforcement: (68, 38)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (68,38) (43,) (68,38) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m scaler\u001b[38;5;241m.\u001b[39mfeature_names_in_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(df_new\u001b[38;5;241m.\u001b[39mcolumns)  \u001b[38;5;66;03m# ‚úÖ Force correct feature names\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# ‚úÖ Apply the same scaling transformation used during training\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m df_new_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(df_new)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# ‚úÖ Debugging Step: Print the shape before prediction\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Shape of df_new_scaled BEFORE prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_new_scaled\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n\u001b[0;32m-> 1062\u001b[0m         X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_std:\n\u001b[1;32m   1064\u001b[0m         X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (68,38) (43,) (68,38) "
     ]
    }
   ],
   "source": [
    "# ‚úÖ Debugging Step: Print all columns BEFORE selection\n",
    "print(\"\\n‚úÖ Features in df_new BEFORE selection:\")\n",
    "print(df_new.columns.tolist())\n",
    "\n",
    "# ‚úÖ Manually enforce correct feature selection\n",
    "expected_features = list(scaler.feature_names_in_)\n",
    "\n",
    "# ‚úÖ Identify extra features\n",
    "extra_features = [col for col in df_new.columns if col not in expected_features]\n",
    "if extra_features:\n",
    "    print(f\"\\nüö® Dropping extra features: {extra_features}\")\n",
    "    df_new = df_new.drop(columns=extra_features, errors=\"ignore\")  # ‚úÖ Force drop extra features\n",
    "\n",
    "# ‚úÖ Debugging Step: Print all columns AFTER selection\n",
    "print(\"\\n‚úÖ Features in df_new AFTER dropping extras:\")\n",
    "print(df_new.columns.tolist())\n",
    "\n",
    "# ‚úÖ Adjust `expected_features` to only include columns that still exist in df_new\n",
    "expected_features = [col for col in expected_features if col in df_new.columns]\n",
    "\n",
    "# ‚úÖ Reorder columns to match expected order\n",
    "df_new = df_new[expected_features]\n",
    "\n",
    "# ‚úÖ Debugging Step: Print final shape before scaling\n",
    "print(f\"‚úÖ Final df_new shape after feature enforcement: {df_new.shape}\")\n",
    "\n",
    "# ‚úÖ Manually adjust scaler to recognize only the 42 features that remain\n",
    "scaler.n_features_in_ = len(df_new.columns)  # ‚úÖ Force correct feature count\n",
    "scaler.feature_names_in_ = np.array(df_new.columns)  # ‚úÖ Force correct feature names\n",
    "\n",
    "# ‚úÖ Apply the same scaling transformation used during training\n",
    "df_new_scaled = scaler.transform(df_new)\n",
    "\n",
    "# ‚úÖ Debugging Step: Print the shape before prediction\n",
    "print(f\"‚úÖ Shape of df_new_scaled BEFORE prediction: {df_new_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82722a82-424b-4eb5-850d-66b7be8680ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns used during training\n",
    "feature_columns = scaler.feature_names_in_  # This ensures we use the same features\n",
    "\n",
    "# Check if new dataset has the required columns\n",
    "missing_cols = [col for col in feature_columns if col not in df_new.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing columns in new dataset: {missing_cols}\")\n",
    "\n",
    "# Check the mean and standard deviation of training data (originally scaled)\n",
    "train_mean = scaler.mean_\n",
    "train_std = scaler.scale_\n",
    "\n",
    "# Check the mean and standard deviation of the new dataset BEFORE scaling\n",
    "new_mean = df_new[feature_columns].mean().values\n",
    "new_std = df_new[feature_columns].std().values\n",
    "\n",
    "# Display both side by side for comparison\n",
    "print(\"\\nüîç Comparing Mean and Std. Dev. (Training vs. New Data)\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Feature\": feature_columns,\n",
    "    \"Train Mean\": train_mean,\n",
    "    \"New Data Mean\": new_mean,\n",
    "    \"Train Std\": train_std,\n",
    "    \"New Data Std\": new_std\n",
    "})\n",
    "display(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d3891-e0ee-4cd7-a518-97f2bf37d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Load trained neural network model BEFORE making predictions\n",
    "model = load_model(\"../L2/data/elite8_nn_model.h5\")  # ‚úÖ Ensure this line is present\n",
    "\n",
    "# ‚úÖ Make predictions\n",
    "predictions = model.predict(df_new_scaled)\n",
    "\n",
    "# ‚úÖ Convert probabilities to DataFrame column\n",
    "df_new[\"Elite 8 Probability\"] = predictions.flatten()\n",
    "\n",
    "# ‚úÖ Adjust threshold for classification\n",
    "new_threshold = 0.4  # Adjust as needed (try 0.4 first, then 0.35 if necessary)\n",
    "df_new[\"Elite 8 Prediction\"] = (df_new[\"Elite 8 Probability\"] >= new_threshold).astype(int)\n",
    "\n",
    "print(f\"\\nüîç Applied new threshold: {new_threshold}\")\n",
    "\n",
    "##############\n",
    "print(\"\\nüîç Checking Model Predictions with New Threshold:\")\n",
    "print(df_new[[\"Elite 8 Probability\", \"Elite 8 Prediction\"]]\n",
    "      .sort_values(by=\"Elite 8 Probability\", ascending=False)\n",
    "      .head(20))  # Show top 20 teams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(df_new[\"Elite 8 Probability\"], bins=20, color=\"blue\", alpha=0.7, edgecolor=\"black\")\n",
    "plt.xlabel(\"Elite 8 Probability\")\n",
    "plt.ylabel(\"Number of Teams\")\n",
    "plt.title(\"Distribution of Model's Predicted Probabilities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81389876-28bd-437d-9a7e-5b98443fcef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Ensure `df_new_final` is a copy of `df_new`\n",
    "df_new_final = df_new.copy()\n",
    "\n",
    "# ‚úÖ Ensure the \"Team\" column is correctly reattached\n",
    "df_new_final[\"Team\"] = df_new_team[\"Team\"].values  # Use `.values` to avoid index mismatch\n",
    "\n",
    "# ‚úÖ Keep only relevant columns\n",
    "df_new_final = df_new_final[[\"Team\", \"Elite 8 Probability\", \"Elite 8 Prediction\"]]\n",
    "\n",
    "# ‚úÖ Sort by highest probability\n",
    "df_new_final = df_new_final.sort_values(by=\"Elite 8 Probability\", ascending=False)\n",
    "\n",
    "# Display the top 8 teams in a readable format\n",
    "display(df_new_final.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63893a-57f1-4c04-9f18-4bc2201f92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save results to CSV\n",
    "output_filename = \"../L2/data/barttorvik_predict2025NN_L2.csv\"\n",
    "df_new_final.to_csv(output_filename, index=False)\n",
    "\n",
    "# Display confirmation\n",
    "print(f\"Predictions saved to {output_filename} ‚úÖ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
