"""
L4.01 - Model Agreement Validation

Compares Elite 8 model (direct predictions from L3) with H2H model
(Monte Carlo tournament simulation) to determine integration strategy.

Data contracts:
  - Elite 8: pre-computed by L3/elite8/05_apply_to_2026.py → load from CSV
  - H2H: 29 pct_diff_* features computed on-the-fly from raw columns in predict_set.
    Feature order and source columns are defined by ensemble_config.json.
    Scaler was fit on that exact order.
  - Bracket: derived from tournamentSeed in predict_set_2026.csv

Outputs:
  - disagreement_analysis.csv
  - validation_summary.json
  - simulation_stats.json
  - model_agreement_scatter.png
  - top_teams_comparison.png
  - simulation_summary.png
"""

import pandas as pd
import numpy as np
import pickle
import json
from pathlib import Path
from scipy.stats import pearsonr, spearmanr
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURATION
# ============================================================================

# L3 output paths — source of truth
ELITE8_PREDICTIONS_PATH = Path("../L3/elite8/outputs/05_2026_predictions/elite8_predictions_2026_long.csv")
H2H_MODEL_DIR            = Path("../L3/h2h/models")
PREDICTION_DATA_PATH     = Path("../L3/data/predictionData/predict_set_2026.csv")

# L4 output
OUTPUT_DIR = Path("outputs/01_validation")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Simulation
N_SIMULATIONS = 5000
RANDOM_SEED   = 42
np.random.seed(RANDOM_SEED)

# Agreement thresholds
HIGH_AGREEMENT_THRESHOLD     = 0.85
MODERATE_AGREEMENT_THRESHOLD = 0.70
DISAGREEMENT_THRESHOLD       = 0.15   # flag teams with >15% absolute diff

# Config name → short key mapping
NAME_TO_KEY = {
    'Gradient Boosting':    'GB',
    'SVM':                  'SVM',
    'Random Forest':        'RF',
    'Neural Network':       'NN',
    'Gaussian Naive Bayes': 'GNB'
}

H2H_MODEL_FILES = {
    'GB':  'gradient_boosting.pkl',
    'SVM': 'svm.pkl',
    'RF':  'random_forest.pkl',
    'NN':  'neural_network.pkl',
    'GNB': 'gaussian_naive_bayes.pkl'
}

# ============================================================================
# LOADING
# ============================================================================

def load_elite8_predictions():
    """
    Load pre-computed Elite 8 predictions from L3.
    Generated by 05_apply_to_2026.py using the long/production ensemble.
    No re-prediction needed.
    """
    print("Loading Elite 8 predictions (L3 output)...")
    df = pd.read_csv(ELITE8_PREDICTIONS_PATH)
    print(f"  ✓ {len(df)} teams, P(Elite8) range: "
          f"[{df['Elite8_Probability'].min():.3f}, {df['Elite8_Probability'].max():.3f}]")
    print(f"    Top 5: {list(df.head(5)['Team'])}")
    return df


def load_h2h():
    """
    Load H2H models, scaler, and config.

    Config is the single source of truth for:
      - feature_columns: 29 pct_diff_* names, in training order
      - weights / fallback_weights: ensemble weights

    We derive source_columns by stripping 'pct_diff_' → these are the
    columns we pull from predict_set_2026.csv to compute differentials.
    """
    print("Loading H2H models and config...")

    # --- Config ---
    with open(H2H_MODEL_DIR / 'ensemble_config.json', 'r') as f:
        config = json.load(f)

    feature_columns = config['feature_columns']                          # 29 pct_diff_* names
    source_columns  = [c.replace('pct_diff_', '') for c in feature_columns]  # raw column names

    # Remap weights: full names → short keys
    weights = {NAME_TO_KEY[k]: v for k, v in config['weights'].items()}

    # Remap fallback weights (pre-computed in L3 for GNB exclusion scenarios)
    fallback_weights = {}
    for scenario, w in config.get('fallback_weights', {}).items():
        fallback_weights[scenario] = {NAME_TO_KEY[k]: v for k, v in w.items()}

    print(f"  ✓ Config: {len(feature_columns)} features, "
          f"train years {config['train_years']}")
    print(f"    Weights:  { {k: f'{v:.3f}' for k, v in weights.items()} }")
    print(f"    Fallbacks: {list(fallback_weights.keys())}")

    # --- Models ---
    models = {}
    for key, filename in H2H_MODEL_FILES.items():
        with open(H2H_MODEL_DIR / filename, 'rb') as f:
            models[key] = pickle.load(f)
        print(f"  ✓ Loaded {key}")

    # --- Scaler ---
    with open(H2H_MODEL_DIR / 'feature_scaler.pkl', 'rb') as f:
        scaler = pickle.load(f)
    assert scaler.n_features_in_ == len(feature_columns), (
        f"Scaler expects {scaler.n_features_in_} features, config has {len(feature_columns)}"
    )
    print(f"  ✓ Scaler: {scaler.n_features_in_} features")

    return {
        'models':           models,
        'scaler':           scaler,
        'weights':          weights,
        'fallback_weights': fallback_weights,
        'feature_columns':  feature_columns,   # pct_diff_* (for reference/logging)
        'source_columns':   source_columns,    # raw columns in predict_set
    }


def load_prediction_data():
    """Load raw 2026 prediction features."""
    print("Loading prediction data...")
    df = pd.read_csv(PREDICTION_DATA_PATH)
    print(f"  ✓ {len(df)} teams, {len(df.columns)} columns")
    return df


# ============================================================================
# BRACKET CONSTRUCTION
# ============================================================================

def construct_bracket(prediction_df):
    """
    Build tournament bracket from tournamentSeed.

    - Filters to seeds 1-16 (may be 68 teams if First Four included)
    - Assigns 4 regions round-robin by seed
    - Standard R64 matchups: 1v16, 8v9, 5v12, 4v13, 6v11, 3v14, 7v10, 2v15
      (order matters — R32 pairs consecutive winners)

    Returns:
        bracket: {region: [(team1, team2), ...]} — R64 matchups in bracket order
        bracket_df: filtered DataFrame with Region column added
    """
    print("Constructing bracket...")

    df = prediction_df.copy()
    print(f"  ℹ Filtering seeds 1-16 from {len(df)} total teams")
    df = df[(df['tournamentSeed'] >= 1) & (df['tournamentSeed'] <= 16)].copy()
    print(f"    → {len(df)} tournament teams")

    if len(df) == 0:
        raise ValueError("No teams with seeds 1-16")

    # Sort by seed for deterministic region assignment
    df = df.sort_values('tournamentSeed').reset_index(drop=True)

    # Assign regions round-robin within each seed
    region_names = ['East', 'West', 'South', 'Midwest']
    regions = []
    for seed in sorted(df['tournamentSeed'].unique()):
        seed_teams = df[df['tournamentSeed'] == seed]
        for idx in range(len(seed_teams)):
            regions.append(region_names[idx % len(region_names)])
    df['Region'] = regions

    df = df.sort_values(['Region', 'tournamentSeed']).reset_index(drop=True)

    # R64 matchup pairs — ORDER defines R32/S16 bracket path
    matchup_pairs = [
        (1, 16), (8, 9),   # → R32 game A
        (5, 12), (4, 13),  # → R32 game B
        (6, 11), (3, 14),  # → R32 game C
        (7, 10), (2, 15)   # → R32 game D
    ]
    # R32: A vs B, C vs D  →  S16: AB vs CD

    bracket = {}
    for region in sorted(df['Region'].unique()):
        region_teams = df[df['Region'] == region].sort_values('tournamentSeed')
        bracket[region] = []

        region_matchup_count = 0
        for seed1, seed2 in matchup_pairs:
            t1 = region_teams[region_teams['tournamentSeed'] == seed1]
            t2 = region_teams[region_teams['tournamentSeed'] == seed2]
            if len(t1) > 0 and len(t2) > 0:
                bracket[region].append((t1.iloc[0]['Team'], t2.iloc[0]['Team']))
                region_matchup_count += 1
            else:
                missing = []
                if len(t1) == 0: missing.append(f"seed {seed1}")
                if len(t2) == 0: missing.append(f"seed {seed2}")
                print(f"      ⚠ {region}: missing {', '.join(missing)}")

        print(f"    - {region}: {len(region_teams)} teams, {region_matchup_count} R64 matchups")

    total = sum(len(m) for m in bracket.values())
    print(f"  ✓ Bracket: {len(bracket)} regions, {total} R64 matchups")

    return bracket, df


# ============================================================================
# H2H PREDICTION
# ============================================================================

def compute_h2h_features(team1, team2, prediction_df, source_columns):
    """
    Compute 29 pct_diff features for a matchup, in config order.

    pct_diff = (t1_val - t2_val) / (|t1_val| + |t2_val|)
    → 0 when both values are 0 (safe division)
    """
    t1_row = prediction_df[prediction_df['Team'] == team1]
    t2_row = prediction_df[prediction_df['Team'] == team2]

    if len(t1_row) == 0 or len(t2_row) == 0:
        return None

    t1_vals = t1_row[source_columns].values[0].astype(float)
    t2_vals = t2_row[source_columns].values[0].astype(float)

    denom = np.abs(t1_vals) + np.abs(t2_vals)
    diffs = np.where(denom == 0, 0.0, (t1_vals - t2_vals) / denom)

    return diffs


def predict_h2h_matchup(team1, team2, prediction_df, h2h):
    """
    Predict P(team1 beats team2) using H2H ensemble.

    Flow:
      1. Compute 29 pct_diff features from raw columns (config order)
      2. Scale with H2H scaler (fit on same order)
      3. Predict with each of 5 models
      4. If GNB is extreme (< 0.01 or > 0.99), exclude and use fallback weights
      5. Weighted average over active models

    Returns: (probability, info_dict)
    """
    diffs = compute_h2h_features(team1, team2, prediction_df, h2h['source_columns'])
    if diffs is None:
        return 0.5, {'excluded': [], 'note': 'Unknown team'}

    X = h2h['scaler'].transform(diffs.reshape(1, -1))

    predictions = {}
    excluded = []

    for key, model in h2h['models'].items():
        try:
            if hasattr(model, 'predict_proba'):
                pred = model.predict_proba(X)[0, 1]
            else:
                pred = float(model.predict(X)[0])

            # GNB exclusion: extreme predictions indicate calibration failure
            if key == 'GNB' and (pred < 0.01 or pred > 0.99):
                excluded.append(key)
                continue

            predictions[key] = pred
        except Exception as e:
            excluded.append(key)

    if len(predictions) == 0:
        return 0.5, {'excluded': excluded, 'note': 'All models excluded'}

    # Weight selection: use fallback if GNB excluded, else primary
    if 'GNB' in excluded and 'exclude_gaussian_naive_bayes' in h2h['fallback_weights']:
        weights = h2h['fallback_weights']['exclude_gaussian_naive_bayes']
    else:
        weights = h2h['weights']

    # Weighted average over active models only
    active_w = {k: weights[k] for k in predictions if k in weights}
    total_w  = sum(active_w.values())

    if total_w == 0:
        return 0.5, {'excluded': excluded, 'note': 'Zero weight'}

    prob = sum(predictions[k] * active_w[k] for k in active_w) / total_w

    return prob, {
        'excluded': excluded,
        'predictions': predictions,
        'weights_used': 'fallback' if 'GNB' in excluded else 'primary'
    }


# ============================================================================
# TOURNAMENT SIMULATION
# ============================================================================

def simulate_tournament(bracket, prediction_df, h2h, n_sims=5000):
    """
    Monte Carlo simulation with matchup caching.

    H2H prediction is deterministic for any (team1, team2) pair.
    Only the coin flip (np.random.random() < p) is stochastic.
    → Cache all computed matchup probabilities. Typical tournament
      sees ~2K unique matchups across 5K sims instead of 315K predictions.

    Bracket path:
      R64 (32 games) → R32 (16) → S16 (8)
      Teams that reach S16 = Elite 8 qualifiers.
      Consecutive-winner pairing preserves bracket structure because
      matchup_pairs order encodes it: (1v16,8v9) → R32 game, etc.
    """
    print(f"Running {n_sims} tournament simulations...")

    # Validate source columns exist
    missing = [c for c in h2h['source_columns'] if c not in prediction_df.columns]
    if missing:
        print(f"  ⚠ {len(missing)} source columns missing, will be 0: {missing}")
        for c in missing:
            prediction_df[c] = 0.0
    else:
        print(f"  ✓ All {len(h2h['source_columns'])} source columns present")

    # Initialize Elite 8 counters for bracket teams only
    elite8_counts = {}
    for matchups in bracket.values():
        for t1, t2 in matchups:
            elite8_counts.setdefault(t1, 0)
            elite8_counts.setdefault(t2, 0)

    # Matchup cache: (t1, t2) → p(t1 wins)
    # Deterministic prediction, only coin flip varies per sim
    cache = {}
    gnb_exclusion_count = 0
    total_matchups      = 0

    def get_prob(t1, t2):
        """Cached matchup probability."""
        nonlocal gnb_exclusion_count, total_matchups
        total_matchups += 1

        if (t1, t2) in cache:
            return cache[(t1, t2)]

        p, info = predict_h2h_matchup(t1, t2, prediction_df, h2h)
        cache[(t1, t2)]  = p
        cache[(t2, t1)]  = 1.0 - p   # symmetric

        if 'GNB' in info.get('excluded', []):
            gnb_exclusion_count += 1

        return p

    for _ in tqdm(range(n_sims), desc="Simulations"):
        # --- R64 ---
        r64_winners = {}
        for region, matchups in bracket.items():
            r64_winners[region] = []
            for t1, t2 in matchups:
                p = get_prob(t1, t2)
                r64_winners[region].append(t1 if np.random.random() < p else t2)

        # --- R32: consecutive R64 winners paired by bracket position ---
        r32_winners = {}
        for region, winners in r64_winners.items():
            r32_winners[region] = []
            for i in range(0, len(winners), 2):
                if i + 1 < len(winners):
                    t1, t2 = winners[i], winners[i+1]
                    p = get_prob(t1, t2)
                    r32_winners[region].append(t1 if np.random.random() < p else t2)

        # --- S16: consecutive R32 winners → both reach Elite 8 ---
        for region, winners in r32_winners.items():
            for i in range(0, len(winners), 2):
                if i + 1 < len(winners):
                    t1, t2 = winners[i], winners[i+1]
                    p = get_prob(t1, t2)
                    # Both S16 participants are Elite 8 qualifiers
                    elite8_counts[t1] += 1
                    elite8_counts[t2] += 1

    # Convert to probabilities
    elite8_probs = {team: count / n_sims for team, count in elite8_counts.items()}

    stats = {
        'n_simulations':     n_sims,
        'total_matchups':    total_matchups,
        'unique_matchups':   len(cache) // 2,
        'gnb_exclusions':    gnb_exclusion_count,
        'gnb_exclusion_rate': round(gnb_exclusion_count / max(len(cache) // 2, 1), 4),
        'teams_tracked':     len(elite8_counts),
        'teams_reaching_e8': sum(1 for c in elite8_counts.values() if c > 0)
    }

    print(f"  ✓ Complete")
    print(f"    - Unique matchups predicted: {stats['unique_matchups']:,} "
          f"(cached across {total_matchups:,} total)")
    print(f"    - GNB exclusion rate: {stats['gnb_exclusion_rate']:.1%}")
    print(f"    - Teams reaching Elite 8 at least once: {stats['teams_reaching_e8']}")

    return elite8_probs, stats


# ============================================================================
# AGREEMENT ANALYSIS
# ============================================================================

def analyze_agreement(elite8_direct, elite8_simulated):
    """
    Compare Elite 8 direct predictions vs H2H simulated probabilities.

    Args:
        elite8_direct: DataFrame with Team, Seed, Region, P_Elite8_Direct
        elite8_simulated: dict {team_name: P(Elite8)}

    Returns:
        analysis_df: team-level comparison, sorted by avg probability
        summary: correlation metrics + integration recommendation
    """
    print("Analyzing model agreement...")

    df = elite8_direct.copy()
    df['P_Elite8_Simulated'] = df['Team'].map(elite8_simulated)
    df = df.dropna(subset=['P_Elite8_Direct', 'P_Elite8_Simulated'])

    # Correlation
    pearson_corr, pearson_p   = pearsonr(df['P_Elite8_Direct'], df['P_Elite8_Simulated'])
    spearman_corr, spearman_p = spearmanr(df['P_Elite8_Direct'], df['P_Elite8_Simulated'])

    # Error
    rmse = np.sqrt(mean_squared_error(df['P_Elite8_Direct'], df['P_Elite8_Simulated']))
    mae  = np.mean(np.abs(df['P_Elite8_Direct'] - df['P_Elite8_Simulated']))

    # Per-team differences
    df['Abs_Difference']           = np.abs(df['P_Elite8_Direct'] - df['P_Elite8_Simulated'])
    df['Pct_Difference']           = df['Abs_Difference'] / ((df['P_Elite8_Direct'] + df['P_Elite8_Simulated']) / 2)
    df['Direct_Higher']            = df['P_Elite8_Direct'] > df['P_Elite8_Simulated']
    df['Significant_Disagreement'] = df['Abs_Difference'] > DISAGREEMENT_THRESHOLD

    # Simple ensemble average (weights adjusted in L4.02 based on strategy)
    df['P_Elite8_Ensemble'] = (df['P_Elite8_Direct'] + df['P_Elite8_Simulated']) / 2

    # --- Integration strategy ---
    if pearson_corr >= HIGH_AGREEMENT_THRESHOLD:
        strategy = "HIGH_AGREEMENT"
        recommendation = (
            f"Models agree strongly (r={pearson_corr:.3f}). "
            f"H2H simulation is primary — it captures path-dependent dynamics. "
            f"Elite 8 model validates. Use ensemble avg for edge cases (>10% diff)."
        )
        w_h2h, w_e8 = 0.70, 0.30

    elif pearson_corr >= MODERATE_AGREEMENT_THRESHOLD:
        strategy = "MODERATE_AGREEMENT"
        recommendation = (
            f"Moderate agreement (r={pearson_corr:.3f}). "
            f"Blend both: 0.6 * H2H + 0.4 * Elite8. "
            f"Each model captures signal the other misses."
        )
        w_h2h, w_e8 = 0.60, 0.40

    else:
        strategy = "LOW_AGREEMENT"
        recommendation = (
            f"Low agreement (r={pearson_corr:.3f}). Investigate before blending. "
            f"Consider using H2H for bracket simulation and Elite 8 for Calcutta "
            f"value separately — they may be capturing different things."
        )
        w_h2h, w_e8 = 0.50, 0.50

    summary = {
        'n_teams':                            len(df),
        'pearson_correlation':                float(pearson_corr),
        'pearson_p_value':                    float(pearson_p),
        'spearman_correlation':               float(spearman_corr),
        'spearman_p_value':                   float(spearman_p),
        'rmse':                               float(rmse),
        'mae':                                float(mae),
        'max_absolute_difference':            float(df['Abs_Difference'].max()),
        'median_absolute_difference':         float(df['Abs_Difference'].median()),
        'teams_with_significant_disagreement': int(df['Significant_Disagreement'].sum()),
        'pct_teams_with_disagreement':        float(df['Significant_Disagreement'].mean()),
        'integration_strategy':               strategy,
        'recommendation':                     recommendation,
        'suggested_weight_h2h':               w_h2h,
        'suggested_weight_elite8':            w_e8
    }

    # Sort by average probability
    df['Avg_Probability'] = (df['P_Elite8_Direct'] + df['P_Elite8_Simulated']) / 2
    df = df.sort_values('Avg_Probability', ascending=False).reset_index(drop=True)

    print(f"  ✓ Pearson r={pearson_corr:.4f} | Strategy: {strategy} | "
          f"Disagreements: {summary['teams_with_significant_disagreement']} teams")

    return df, summary


# ============================================================================
# VISUALIZATIONS
# ============================================================================

def create_visualizations(analysis_df, summary, simulation_stats):
    """Save diagnostic plots."""
    print("Creating visualizations...")
    sns.set_style("whitegrid")

    # ----------------------------------------------------------------
    # 1. Scatter: Direct vs Simulated + disagreement distribution
    # ----------------------------------------------------------------
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    colors = ['red' if x else 'steelblue' for x in analysis_df['Significant_Disagreement']]
    axes[0].scatter(analysis_df['P_Elite8_Direct'], analysis_df['P_Elite8_Simulated'],
                    c=colors, alpha=0.7, s=60, edgecolors='black', linewidth=0.5)
    axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1.5, alpha=0.5, label='Perfect agreement')
    axes[0].set_xlabel('Elite 8 Model (Direct)', fontsize=11, fontweight='bold')
    axes[0].set_ylabel('H2H Model (Simulated)', fontsize=11, fontweight='bold')
    axes[0].set_title(
        f'Model Agreement\nr = {summary["pearson_correlation"]:.3f} | '
        f'{summary["integration_strategy"]}',
        fontsize=12, fontweight='bold'
    )
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    axes[0].set_xlim(-0.05, 1.05)
    axes[0].set_ylim(-0.05, 1.05)

    # Annotate top 5 disagreements
    for _, row in analysis_df.nlargest(5, 'Abs_Difference').iterrows():
        axes[0].annotate(
            row['Team'],
            (row['P_Elite8_Direct'], row['P_Elite8_Simulated']),
            fontsize=7, fontweight='bold',
            bbox=dict(boxstyle='round,pad=0.2', facecolor='yellow', alpha=0.6)
        )

    # Difference distribution
    axes[1].hist(analysis_df['Abs_Difference'], bins=25, edgecolor='black',
                 alpha=0.7, color='steelblue')
    axes[1].axvline(DISAGREEMENT_THRESHOLD, color='red', linestyle='--', linewidth=2,
                    label=f'Threshold ({DISAGREEMENT_THRESHOLD:.0%})')
    axes[1].axvline(analysis_df['Abs_Difference'].median(), color='green',
                    linestyle='--', linewidth=2,
                    label=f'Median ({summary["median_absolute_difference"]:.3f})')
    axes[1].set_xlabel('Absolute Difference', fontsize=11, fontweight='bold')
    axes[1].set_ylabel('Frequency', fontsize=11, fontweight='bold')
    axes[1].set_title(f'Disagreement Distribution\nMAE={summary["mae"]:.3f} | RMSE={summary["rmse"]:.3f}',
                      fontsize=12, fontweight='bold')
    axes[1].legend()

    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'model_agreement_scatter.png', dpi=300, bbox_inches='tight')
    print(f"  ✓ model_agreement_scatter.png")
    plt.close()

    # ----------------------------------------------------------------
    # 2. Top 20 teams bar comparison
    # ----------------------------------------------------------------
    top20 = analysis_df.head(20)
    fig, ax = plt.subplots(figsize=(14, 7))
    x = np.arange(len(top20))
    w = 0.35

    bars1 = ax.bar(x - w/2, top20['P_Elite8_Direct'],    w, label='Elite 8 (Direct)',  alpha=0.8, color='steelblue', edgecolor='black')
    bars2 = ax.bar(x + w/2, top20['P_Elite8_Simulated'], w, label='H2H (Simulated)',   alpha=0.8, color='coral',     edgecolor='black')

    ax.set_xlabel('Team', fontsize=11, fontweight='bold')
    ax.set_ylabel('P(Elite 8)', fontsize=11, fontweight='bold')
    ax.set_title('Top 20 Elite 8 Candidates', fontsize=13, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(top20['Team'], rotation=45, ha='right', fontsize=9)
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')
    ax.set_ylim(0, 1.0)

    for bars in [bars1, bars2]:
        for bar in bars:
            h = bar.get_height()
            if h > 0.04:
                ax.text(bar.get_x() + bar.get_width()/2., h, f'{h:.2f}',
                        ha='center', va='bottom', fontsize=7)

    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'top_teams_comparison.png', dpi=300, bbox_inches='tight')
    print(f"  ✓ top_teams_comparison.png")
    plt.close()

    # ----------------------------------------------------------------
    # 3. Summary text card
    # ----------------------------------------------------------------
    fig, ax = plt.subplots(figsize=(10, 6))
    fig.patch.set_facecolor('#f8f9fa')

    info = (
        f"L4.01 Validation Summary\n"
        f"{'─' * 38}\n\n"
        f"Simulations:        {simulation_stats['n_simulations']:,}\n"
        f"Unique matchups:    {simulation_stats['unique_matchups']:,}\n"
        f"GNB exclusion rate: {simulation_stats['gnb_exclusion_rate']:.1%}\n"
        f"Teams to Elite 8:   {simulation_stats['teams_reaching_e8']}\n\n"
        f"{'─' * 38}\n"
        f"Pearson r:          {summary['pearson_correlation']:.4f}\n"
        f"Spearman r:         {summary['spearman_correlation']:.4f}\n"
        f"MAE:                {summary['mae']:.4f}\n"
        f"RMSE:               {summary['rmse']:.4f}\n\n"
        f"{'─' * 38}\n"
        f"Strategy:           {summary['integration_strategy']}\n"
        f"H2H weight:         {summary['suggested_weight_h2h']:.0%}\n"
        f"Elite 8 weight:     {summary['suggested_weight_elite8']:.0%}\n\n"
        f"{summary['recommendation']}"
    )

    ax.text(0.05, 0.95, info, transform=ax.transAxes,
            fontsize=10.5, verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='#ccc'))
    ax.axis('off')

    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'simulation_summary.png', dpi=300, bbox_inches='tight')
    print(f"  ✓ simulation_summary.png")
    plt.close()

    print(f"  ✓ All visualizations saved to {OUTPUT_DIR}/")


# ============================================================================
# MAIN
# ============================================================================

def main():
    print()
    print("=" * 70)
    print("           L4.01 — MODEL AGREEMENT VALIDATION")
    print("=" * 70)
    print()

    # ================================================================
    # STEP 1: Load
    # ================================================================
    print("STEP 1: Loading data and models")
    print("-" * 70)

    elite8_df     = load_elite8_predictions()   # from L3 CSV
    h2h           = load_h2h()                  # models + config
    prediction_df = load_prediction_data()      # raw features

    print(f"\n✓ All inputs loaded\n")

    # ================================================================
    # STEP 2: Bracket
    # ================================================================
    print("STEP 2: Constructing tournament bracket")
    print("-" * 70)

    bracket, bracket_df = construct_bracket(prediction_df)
    print()

    # ================================================================
    # STEP 3: Elite 8 direct predictions (from L3 CSV)
    # ================================================================
    print("STEP 3: Elite 8 direct predictions (L3 output)")
    print("-" * 70)

    tournament_teams = set(bracket_df['Team'])
    elite8_direct = (
        elite8_df[elite8_df['Team'].isin(tournament_teams)]
        [['Team', 'Elite8_Probability']]
        .rename(columns={'Elite8_Probability': 'P_Elite8_Direct'})
        .merge(bracket_df[['Team', 'tournamentSeed', 'Region']], on='Team')
        .rename(columns={'tournamentSeed': 'Seed'})
    )

    print(f"  ✓ {len(elite8_direct)} tournament teams with Elite 8 probabilities")
    print(f"    - P(Elite8) range: [{elite8_direct['P_Elite8_Direct'].min():.3f}, "
          f"{elite8_direct['P_Elite8_Direct'].max():.3f}]")
    top3 = elite8_direct.nlargest(3, 'P_Elite8_Direct')
    for _, row in top3.iterrows():
        print(f"    - {row['Team']}: {row['P_Elite8_Direct']:.3f}")
    print()

    # ================================================================
    # STEP 4: H2H tournament simulation
    # ================================================================
    print("STEP 4: H2H tournament simulation")
    print("-" * 70)

    elite8_simulated, sim_stats = simulate_tournament(
        bracket, prediction_df, h2h, n_sims=N_SIMULATIONS
    )
    print()

    # ================================================================
    # STEP 5: Agreement analysis
    # ================================================================
    print("STEP 5: Analyzing model agreement")
    print("-" * 70)

    analysis_df, summary = analyze_agreement(elite8_direct, elite8_simulated)
    print()

    # ================================================================
    # STEP 6: Save outputs
    # ================================================================
    print("STEP 6: Saving outputs")
    print("-" * 70)

    analysis_df.to_csv(OUTPUT_DIR / 'disagreement_analysis.csv', index=False)
    print(f"  ✓ disagreement_analysis.csv ({len(analysis_df)} teams)")

    with open(OUTPUT_DIR / 'validation_summary.json', 'w') as f:
        json.dump(summary, f, indent=2)
    print(f"  ✓ validation_summary.json")

    with open(OUTPUT_DIR / 'simulation_stats.json', 'w') as f:
        json.dump(sim_stats, f, indent=2)
    print(f"  ✓ simulation_stats.json")

    create_visualizations(analysis_df, summary, sim_stats)
    print()

    # ================================================================
    # RESULTS
    # ================================================================
    print("=" * 70)
    print("                    VALIDATION SUMMARY")
    print("=" * 70)
    print()
    print(f"  Correlation:   Pearson  {summary['pearson_correlation']:>6.4f}  "
          f"(p={summary['pearson_p_value']:.2e})")
    print(f"                 Spearman {summary['spearman_correlation']:>6.4f}  "
          f"(p={summary['spearman_p_value']:.2e})")
    print(f"  Error:         MAE      {summary['mae']:>6.4f}")
    print(f"                 RMSE     {summary['rmse']:>6.4f}")
    print(f"                 Max Δ    {summary['max_absolute_difference']:>6.4f}")
    print(f"  Disagreements: {summary['teams_with_significant_disagreement']} teams "
          f"({summary['pct_teams_with_disagreement']:.0%})")
    print()
    print(f"  Strategy: {summary['integration_strategy']}")
    print(f"  Weights:  H2H {summary['suggested_weight_h2h']:.0%} / "
          f"Elite8 {summary['suggested_weight_elite8']:.0%}")
    print(f"  → {summary['recommendation']}")
    print()

    # Top disagreements
    print("=" * 70)
    print("                     TOP 10 DISAGREEMENTS")
    print("=" * 70)
    print()
    print(f"  {'Team':<18} {'Direct':<9} {'Simulated':<10} {'Δ':<8} {'Direction'}")
    print(f"  {'-'*18} {'-'*9} {'-'*10} {'-'*8} {'-'*15}")
    for _, row in analysis_df.nlargest(10, 'Abs_Difference').iterrows():
        direction = "E8 ↑" if row['Direct_Higher'] else "H2H ↑"
        print(f"  {row['Team']:<18} {row['P_Elite8_Direct']:>6.1%}   "
              f"{row['P_Elite8_Simulated']:>6.1%}     "
              f"{row['Abs_Difference']:>5.1%}   {direction}")
    print()

    # Top Elite 8 candidates
    print("=" * 70)
    print("                  TOP 10 ELITE 8 CANDIDATES")
    print("=" * 70)
    print()
    print(f"  {'Team':<18} {'Seed':<5} {'Direct':<9} {'Simulated':<10} {'Ensemble'}")
    print(f"  {'-'*18} {'-'*5} {'-'*9} {'-'*10} {'-'*9}")
    for _, row in analysis_df.head(10).iterrows():
        print(f"  {row['Team']:<18} {int(row['Seed']):<5} {row['P_Elite8_Direct']:>6.1%}   "
              f"{row['P_Elite8_Simulated']:>6.1%}     {row['P_Elite8_Ensemble']:>6.1%}")
    print()

    # Next steps
    print("=" * 70)
    print("                        NEXT STEPS")
    print("=" * 70)
    print()
    if summary['integration_strategy'] == 'HIGH_AGREEMENT':
        print("  → HIGH AGREEMENT: proceed to L4.02 confidently")
        print("  → H2H simulation is primary, Elite 8 validates")
    elif summary['integration_strategy'] == 'MODERATE_AGREEMENT':
        print("  → MODERATE AGREEMENT: blend both in L4.02")
        print(f"  → Formula: {summary['suggested_weight_h2h']:.0%} H2H + "
              f"{summary['suggested_weight_elite8']:.0%} Elite8")
    else:
        print("  ⚠ LOW AGREEMENT: investigate before proceeding")
        print("  → Consider separate use cases: H2H for bracket, E8 for Calcutta")
    print()
    print("  Next: L4.02 — Bracket Simulator")
    print("=" * 70)


if __name__ == "__main__":
    main()
